import streamlit as st
from google import genai
import base64
import json
import requests
import streamlit.components.v1 as components
import os
import time

# =========================================
#  ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…ƒã®ã¾ã¾ï¼‰
# =========================================
SYSTEM_PROMPT = """
ã‚ãªãŸã¯ä¸€äººã®æ—¥æœ¬äººå¥³æ€§ã®ç–‘ä¼¼æ•™å¸«ã§ã‚ã‚Šã€ã€Œãƒ¦ãƒƒã‚­ãƒ¼ã€ã¨åä¹—ã£ã¦ã„ã¾ã™ã€‚
...
ï¼ˆã‚ãªãŸã®å…ƒã‚³ãƒ¼ãƒ‰ã® SYSTEM_PROMPT ã‚’ãã®ã¾ã¾è²¼ã£ã¦ãã ã•ã„ï¼‰
"""

# =========================================
# APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
# =========================================
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except:
    API_KEY = ""

# =========================================
# TTSï¼ˆéŸ³å£°ç”Ÿæˆï¼‰é–¢æ•°
# =========================================
def generate_and_store_tts(text):
    """
    Gemini Flash 2.0 (tts-1) ã«ã‚ˆã‚‹æ—¥æœ¬èªéŸ³å£°ç”Ÿæˆã€‚
    """
    if not text:
        return None

    try:
        client = genai.Client(api_key=API_KEY)

        response = client.models.generate_content(
            model="gemini-2.0-flash-exp",
            contents=[text],
            config={
                "audio_config": {
                    "voice_name": "ja-JP-Neural2-B",
                    "speaking_rate": 1.05
                }
            }
        )

        audio_data = None
        for part in response.parts:
            if hasattr(part, "data") and part.data:
                audio_data = part.data
                break

        if not audio_data:
            print("éŸ³å£°ãƒ‘ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None

        audio_bytes = audio_data
        audio_dir = "generated_audio"
        os.makedirs(audio_dir, exist_ok=True)
        audio_path = os.path.join(
            audio_dir,
            f"tts_{int(time.time())}.wav"
        )

        with open(audio_path, "wb") as f:
            f.write(audio_bytes)

        print("TTS éŸ³å£°ç”ŸæˆæˆåŠŸ:", audio_path)
        return audio_path

    except Exception as e:
        print("TTSç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼:", e)
        return None


# =========================================
# Streamlit UI è¨­å®š
# =========================================
st.set_page_config(
    page_title="ãƒ¦ãƒƒã‚­ãƒ¼",
    layout="wide"   # â˜…ã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒç„¡ã„å‰æã§å…¨å¹…ä½¿ç”¨
)

# ---- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ– ----
if "client" not in st.session_state:
    st.session_state.client = genai.Client(api_key=API_KEY) if API_KEY else None

if "chat" not in st.session_state:
    if st.session_state.client:
        config = {
            "system_instruction": SYSTEM_PROMPT,
            "temperature": 0.2
        }
        st.session_state.chat = st.session_state.client.chats.create(
            model="gemini-2.5-flash",
            config=config
        )
    else:
        st.session_state.chat = None

if "messages" not in st.session_state:
    st.session_state.messages = []

if "audio_to_play" not in st.session_state:
    st.session_state.audio_to_play = None


# =========================================
# ãƒ¡ã‚¤ãƒ³ç”»é¢ UI
# =========================================
st.title("ğŸ€ ãƒ¦ãƒƒã‚­ãƒ¼ï¼ˆç–‘ä¼¼æ•™å¸«ï¼‰")
st.caption("çŸ¥è­˜ã¯ç­”ãˆã€æ€è€ƒã¯è§£æ³•ã‚¬ã‚¤ãƒ‰ã®ã¿ã‚’è¿”ã—ã¾ã™ã€‚")

# ---------- ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ----------
st.subheader("ç”»åƒã‚’é€ã£ã¦è³ªå•ã™ã‚‹")

uploaded_image = st.file_uploader("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã¿ã‚ˆã†", type=["jpg", "jpeg", "png"])

if uploaded_image:
    st.image(uploaded_image, caption="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸç”»åƒ", use_column_width=True)
    uploaded_bytes = uploaded_image.read()
else:
    uploaded_bytes = None

# ---------- ãƒ¦ãƒ¼ã‚¶ãƒ¼éŸ³å£°å…¥åŠ› UIï¼ˆWeb Audio APIï¼‰ ----------
components.html("""
<script>
function startVoiceRecognition() {
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    recognition.lang = "ja-JP";
    recognition.onresult = function(event) {
        const text = event.results[0][0].transcript;
        window.parent.postMessage({type: 'stt-result', text: text}, '*');
    };
    recognition.start();
}
</script>

<button onclick="startVoiceRecognition()" style="
    background-color:#ff8dc7;
    border:none;
    padding:12px 18px;
    border-radius:8px;
    color:white;
    font-size:16px;
    cursor:pointer;
    margin-bottom:10px;
">
ğŸ¤ éŸ³å£°ã§è©±ã™
</button>
""", height=80)

# ---------- éŸ³å£°ã§èªè­˜ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®å—ä¿¡ ----------
components.html("""
<script>
window.addEventListener("message", (event) => {
    if (event.data.type === "stt-result") {
        const text = event.data.text;
        window.parent.postMessage({ type: "streamlit:setChatInputValue", value: text }, "*");
        window.parent.postMessage({ type: "streamlit:focusChatInput" }, "*");
    }
});
</script>
""", height=0)

# ---------- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ ----------
st.subheader("ãƒ¦ãƒƒã‚­ãƒ¼ã¨ã®ä¼šè©±å±¥æ­´")

for msg in st.session_state.messages:
    avatar_icon = "ğŸ§‘" if msg["role"] == "user" else "ğŸ¤–"
    with st.chat_message(msg["role"], avatar=avatar_icon):
        st.markdown(msg["content"])

# ---------- ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒƒãƒˆå…¥åŠ› ----------
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„â€¦"):
    # å±¥æ­´ã¸è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ãƒ•ã‚¡ã‚¤ãƒ«ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    file_message = {
        "mime_type": uploaded_image.type if uploaded_image else "text/plain",
        "data": base64.b64encode(uploaded_bytes).decode("utf-8") if uploaded_image else prompt
    }

    # ---- Gemini ã¸é€ä¿¡ ----
if st.session_state.chat:

    # ç”»åƒãŒã‚ã‚‹å ´åˆ
    if uploaded_image:
        response = st.session_state.chat.send_message(
            [
                prompt,
                {
                    "mime_type": uploaded_image.type,
                    "data": uploaded_bytes
                }
            ]
        )

    # ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã®å ´åˆ
    else:
        response = st.session_state.chat.send_message(prompt)

    response_text = response.text if hasattr(response, "text") else str(response)
else:
    response_text = "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚å¿œç­”ã§ãã¾ã›ã‚“ã€‚"

    # å±¥æ­´ã«è¿½åŠ 
    st.session_state.messages.append({"role": "assistant", "content": response_text})

    # TTSç”Ÿæˆ
    audio_path = generate_and_store_tts(response_text)
    if audio_path:
        st.session_state.audio_to_play = audio_path

    st.rerun()

# ---------- éŸ³å£°å†ç”Ÿ ----------
if st.session_state.audio_to_play:
    st.audio(st.session_state.audio_to_play, format="audio/wav")
