import streamlit as st
from google import genai
import base64
import json
import time
import requests
import streamlit.components.v1 as components

# -----------------------------------------------------
# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# -----------------------------------------------------
SYSTEM_PROMPT = """
ã‚ãªãŸã¯æ•™è‚²çš„ãªç›®çš„ã‚’æŒã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦3ã¤ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚

1ï¸âƒ£ çŸ¥è­˜ãƒ»å®šç¾©ã¯ç›´æ¥ç­”ãˆã‚‹ã€‚
2ï¸âƒ£ æ€è€ƒãƒ»è¨ˆç®—å•é¡Œã¯ç­”ãˆã‚’æ•™ãˆãšã€è§£æ³•ã®ãƒ’ãƒ³ãƒˆã®ã¿ã€‚
3ï¸âƒ£ é€”ä¸­å¼ã‚’è¦‹ã›ã‚‰ã‚ŒãŸå ´åˆã¯æ­£èª¤ã‚’åˆ¤å®šã—ã€å„ªã—ãå°ãã€‚
"""

# -----------------------------------------------------
# å…±é€šè¨­å®š
# -----------------------------------------------------
TTS_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent"
TTS_MODEL = "gemini-2.5-flash-preview-tts"
TTS_VOICE = "Kore"
MAX_RETRIES = 5

try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloudã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# -----------------------------------------------------
# éŸ³å£°å†ç”Ÿé–¢æ•°
# -----------------------------------------------------
@st.cache_data
def base64_to_audio_url(base64_data, sample_rate):
    js_code = f"""
    <script>
    function base64ToArrayBuffer(base64){{
        const binary_string = window.atob(base64);
        const len = binary_string.length;
        const bytes = new Uint8Array(len);
        for(let i=0;i<len;i++) bytes[i]=binary_string.charCodeAt(i);
        return bytes.buffer;
    }}
    function pcmToWav(pcmData, sampleRate){{
        const numChannels=1, bitsPerSample=16, bytesPerSample=bitsPerSample/8;
        const blockAlign=numChannels*bytesPerSample, byteRate=sampleRate*blockAlign;
        const dataSize=pcmData.byteLength;
        const buffer=new ArrayBuffer(44+dataSize);
        const view=new DataView(buffer);
        let offset=0;
        function writeString(v,o,s){{for(let i=0;i<s.length;i++)v.setUint8(o+i,s.charCodeAt(i));}}
        writeString(view,offset,'RIFF'); offset+=4;
        view.setUint32(offset,36+dataSize,true); offset+=4;
        writeString(view,offset,'WAVE'); offset+=4;
        writeString(view,offset,'fmt '); offset+=4;
        view.setUint32(offset,16,true); offset+=4;
        view.setUint16(offset,1,true); offset+=2;
        view.setUint16(offset,numChannels,true); offset+=2;
        view.setUint32(offset,sampleRate,true); offset+=4;
        view.setUint32(offset,byteRate,true); offset+=4;
        view.setUint16(offset,blockAlign,true); offset+=2;
        view.setUint16(offset,bitsPerSample,true); offset+=2;
        writeString(view,offset,'data'); offset+=4;
        view.setUint32(offset,dataSize,true); offset+=4;
        const pcm16=new Int16Array(pcmData);
        for(let i=0;i<pcm16.length;i++){{view.setInt16(offset,pcm16[i],true); offset+=2;}}
        return new Blob([buffer],{{type:'audio/wav'}});
    }}
    const pcmData=base64ToArrayBuffer('{base64_data}');
    const wavBlob=pcmToWav(pcmData,{sample_rate});
    const audioUrl=URL.createObjectURL(wavBlob);
    const audio=new Audio(audioUrl);
    audio.play().catch(e=>console.log("Autoplay error:",e));
    </script>
    """
    components.html(js_code, height=0, width=0)

def generate_and_play_tts(text):
    payload = {
        "contents": [{"parts": [{"text": text}]}],
        "generationConfig": {
            "responseModalities": ["AUDIO"],
            "speechConfig":{"voiceConfig":{"prebuiltVoiceConfig":{"voiceName":TTS_VOICE}}}
        },
        "model": TTS_MODEL
    }
    headers={'Content-Type':'application/json'}

    for _ in range(MAX_RETRIES):
        try:
            response=requests.post(f"{TTS_API_URL}?key={API_KEY}",headers=headers,data=json.dumps(payload))
            response.raise_for_status()
            result=response.json()
            audio_data=result["candidates"][0]["content"]["parts"][0].get("inlineData",{})
            if "data" in audio_data:
                mime_type=audio_data.get("mimeType","audio/L16;rate=24000")
                rate=int(mime_type.split("rate=")[1]) if "rate=" in mime_type else 24000
                base64_to_audio_url(audio_data["data"], rate)
                return
        except Exception as e:
            st.write("TTSã‚¨ãƒ©ãƒ¼:", e)
            time.sleep(1)

# -----------------------------------------------------
# éŸ³å£°èªè­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
# -----------------------------------------------------
def speech_to_text_ui():
    st.markdown("### ğŸ¤ è©±ã—ã¦è³ªå•ï¼ˆéŸ³å£°èªè­˜ï¼‰")

    html_code = """
    <script>
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;
    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.lang = 'ja-JP';
        recognition.interimResults = false;
        recognition.continuous = false;

        function startRecognition() {
            document.getElementById('mic-status').innerText = 'ğŸ§ è´ãå–ã‚Šä¸­...';
            recognition.start();
        }

        recognition.onresult = function(event) {
            const transcript = event.results[0][0].transcript;
            document.getElementById('mic-status').innerText = 'âœ… èªè­˜å®Œäº†: ' + transcript;
            const streamlitInput = window.parent;
            streamlitInput.postMessage(
                { type: 'streamlit:setComponentValue', value: transcript },
                '*'
            );
        }

        recognition.onerror = function(event) {
            document.getElementById('mic-status').innerText = 'âš ï¸ ã‚¨ãƒ©ãƒ¼: ' + event.error;
        }
    } else {
        document.getElementById('mic-status').innerText = 'ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«éå¯¾å¿œã§ã™ã€‚';
    }
    </script>

    <button onclick="startRecognition()">ğŸ™ è©±ã™</button>
    <p id="mic-status">ãƒã‚¤ã‚¯åœæ­¢ä¸­</p>
    """
    result = components.html(html_code, height=120)
    return result

# -----------------------------------------------------
# Streamlitæœ¬ä½“
# -----------------------------------------------------
st.set_page_config(page_title="ãƒ¦ãƒƒã‚­ãƒ¼", layout="wide")
st.title("ãƒ¦ãƒƒã‚­ãƒ¼")
st.caption("ğŸ“ è©±ã—ã‹ã‘ã‚‹ã ã‘ã§è³ªå•ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")

# Gemini åˆæœŸåŒ–
if "client" not in st.session_state:
    st.session_state.client=genai.Client(api_key=API_KEY)
if "chat" not in st.session_state:
    config={"system_instruction":SYSTEM_PROMPT,"temperature":0.2}
    st.session_state.chat=st.session_state.client.chats.create(model='gemini-2.5-flash',config=config)

USER_AVATAR="ğŸ§‘"
AI_AVATAR="yukki-icon.jpg"

if "messages" not in st.session_state:
    st.session_state.messages=[]

# å±¥æ­´è¡¨ç¤º
for msg in st.session_state.messages:
    avatar=USER_AVATAR if msg["role"]=="user" else AI_AVATAR
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])

# éŸ³å£°èªè­˜ã‹ã‚‰ã®å…¥åŠ›
spoken_text = speech_to_text_ui()

# Chatå…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...") or st.session_state.get("spoken_text"):
    if prompt:
        st.session_state.messages.append({"role":"user","content":prompt})
        with st.chat_message("user", avatar=USER_AVATAR):
            st.markdown(prompt)

        with st.chat_message("assistant", avatar=AI_AVATAR):
            with st.spinner("è€ƒãˆä¸­..."):
                try:
                    response = st.session_state.chat.send_message(prompt)
                    text = response.text
                    st.markdown(text)
                    st.info("ğŸ”Š éŸ³å£°å‡ºåŠ›ä¸­...")
                    generate_and_play_tts(text)
                    st.session_state.messages.append({"role":"assistant","content":text})
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
