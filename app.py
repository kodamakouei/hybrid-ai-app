import streamlit as st
from google import genai
import base64, json, time, requests
import streamlit.components.v1 as components

# ===============================
# è¨­å®š
# ===============================
SYSTEM_PROMPT = """
ã‚ãªãŸã¯æ•™è‚²çš„ãªç›®çš„ã‚’æŒã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦3ã¤ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å¿œç­”ã—ã¦ãã ã•ã„ã€‚

1ï¸âƒ£ çŸ¥è­˜ãƒ»å®šç¾©ã¯ç›´æ¥ç­”ãˆã‚‹ã€‚
2ï¸âƒ£ æ€è€ƒãƒ»è¨ˆç®—å•é¡Œã¯ç­”ãˆã‚’æ•™ãˆãšã€è§£æ³•ã®ãƒ’ãƒ³ãƒˆã®ã¿ã€‚
3ï¸âƒ£ é€”ä¸­å¼ã‚’è¦‹ã›ã‚‰ã‚ŒãŸå ´åˆã¯æ­£èª¤ã‚’åˆ¤å®šã—ã€å„ªã—ãå°ãã€‚
"""
TTS_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent"
TTS_MODEL = "gemini-2.5-flash-preview-tts"
TTS_VOICE = "Kore"
API_KEY = st.secrets["GEMINI_API_KEY"]

# ===============================
# éŸ³å£°å†ç”Ÿ
# ===============================
def base64_to_audio_url(base64_data, sample_rate):
    js_code = f"""
    <script>
    function base64ToArrayBuffer(base64){{
        const binary_string = window.atob(base64);
        const len = binary_string.length;
        const bytes = new Uint8Array(len);
        for(let i=0;i<len;i++) bytes[i]=binary_string.charCodeAt(i);
        return bytes.buffer;
    }}
    function pcmToWav(pcmData, sampleRate){{
        const numChannels=1, bitsPerSample=16, bytesPerSample=bitsPerSample/8;
        const blockAlign=numChannels*bytesPerSample, byteRate=sampleRate*blockAlign;
        const dataSize=pcmData.byteLength;
        const buffer=new ArrayBuffer(44+dataSize);
        const view=new DataView(buffer);
        let offset=0;
        function writeString(v,o,s){{for(let i=0;i<s.length;i++)v.setUint8(o+i,s.charCodeAt(i));}}
        writeString(view,offset,'RIFF'); offset+=4;
        view.setUint32(offset,36+dataSize,true); offset+=4;
        writeString(view,offset,'WAVE'); offset+=4;
        writeString(view,offset,'fmt '); offset+=4;
        view.setUint32(offset,16,true); offset+=4;
        view.setUint16(offset,1,true); offset+=2;
        view.setUint16(offset,numChannels,true); offset+=2;
        view.setUint32(offset,sampleRate,true); offset+=4;
        view.setUint32(offset,byteRate,true); offset+=4;
        view.setUint16(offset,blockAlign,true); offset+=2;
        view.setUint16(offset,bitsPerSample,true); offset+=2;
        writeString(view,offset,'data'); offset+=4;
        view.setUint32(offset,dataSize,true); offset+=4;
        const pcm16=new Int16Array(pcmData);
        for(let i=0;i<pcm16.length;i++){{view.setInt16(offset,pcm16[i],true); offset+=2;}}
        return new Blob([buffer],{{type:'audio/wav'}});
    }}
    const pcmData=base64ToArrayBuffer('{base64_data}');
    const wavBlob=pcmToWav(pcmData,{sample_rate});
    const audioUrl=URL.createObjectURL(wavBlob);
    const audio=new Audio(audioUrl);
    audio.play().catch(e=>console.log("Autoplay error:",e));
    </script>
    """
    components.html(js_code, height=0, width=0)

def generate_and_play_tts(text):
    payload = {
        "contents": [{"parts": [{"text": text}]}],
        "generationConfig": {
            "responseModalities": ["AUDIO"],
            "speechConfig": {"voiceConfig": {"prebuiltVoiceConfig": {"voiceName": TTS_VOICE}}}
        },
        "model": TTS_MODEL
    }
    headers = {'Content-Type': 'application/json'}
    response = requests.post(f"{TTS_API_URL}?key={API_KEY}", headers=headers, data=json.dumps(payload))
    result = response.json()

    # âœ… æ–°ã—ã„æ§‹é€ ã«å¯¾å¿œ
    try:
        audio_data = result["contents"][0]["parts"][0]["inlineData"]
    except KeyError:
        st.error("âŒ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.json(result)
        return

    if "data" in audio_data:
        mime_type = audio_data.get("mimeType", "audio/L16;rate=24000")
        rate = int(mime_type.split("rate=")[1]) if "rate=" in mime_type else 24000
        base64_to_audio_url(audio_data["data"], rate)

# ===============================
# Streamlit UI
# ===============================

# â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ 1: ã‚¢ãƒã‚¿ãƒ¼ã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ CSSã‚’æ³¨å…¥ â˜…â˜…â˜…
# CSSã¯ã€Streamlitã®ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…ã®ç”»åƒï¼ˆã‚¢ãƒã‚¿ãƒ¼ï¼‰ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã‚µã‚¤ã‚ºã‚’64pxã«å›ºå®šã—ã¾ã™ã€‚
st.markdown("""
<style>
/* Chat Message Avatar Image (User and Assistant) */
div[data-testid="stChatMessage"] img {
    width: 10000px !important;
    height: 10000px !important;
    min-width: 10000px !important;
    min-height: 10000px !important;
    object-fit: cover !important; 
}
/* ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒã‚¿ãƒ¼ï¼ˆçµµæ–‡å­—ï¼‰ã‚’å¤§ããè¦‹ã›ã‚‹ãŸã‚ã®èª¿æ•´ */
div[data-testid="stChatMessage"] .st-emotion-cache-1f1f2x2 {
    font-size: 38px !important; 
}

</style>
""", unsafe_allow_html=True)
# â˜…â˜…â˜… ä¿®æ­£ç®‡æ‰€ 1 çµ‚äº† â˜…â˜…â˜…

st.set_page_config(page_title="ãƒ¦ãƒƒã‚­ãƒ¼", layout="wide")
st.title("ğŸ“ ãƒ¦ãƒƒã‚­ãƒ¼ï¼ˆéŸ³å£°å…¥åŠ›å¯¾å¿œï¼‰")

if "client" not in st.session_state:
    st.session_state.client = genai.Client(api_key=API_KEY)
if "chat" not in st.session_state:
    config = {"system_instruction": SYSTEM_PROMPT, "temperature": 0.2}
    st.session_state.chat = st.session_state.client.chats.create(model="gemini-2.5-flash", config=config)
if "messages" not in st.session_state:
    st.session_state.messages = []

# ===============================
# éŸ³å£°å…¥åŠ›ãƒœã‚¿ãƒ³ï¼ˆğŸ™è©±ã™â†’è³ªå•æ¬„ã«å…¥åŠ›ï¼†è‡ªå‹•é€ä¿¡ï¼‰
# ===============================
components.html("""
<script>
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
let recognition;

if (SpeechRecognition) {
    recognition = new SpeechRecognition();
    recognition.lang = 'ja-JP';
    recognition.continuous = false;
    recognition.interimResults = false;

    function startRec() {
        document.getElementById("mic-status").innerText = "ğŸ§ è´ãå–ã‚Šä¸­...";
        recognition.start();
    }

    recognition.onresult = (event) => {
        const text = event.results[0][0].transcript;
        document.getElementById("mic-status").innerText = "âœ… " + text;

        // Streamlitã®è³ªå•æ¬„ï¼ˆchat_inputï¼‰ã‚’æ¢ã™
        const chatInput = window.parent.document.querySelector('textarea[data-testid="stChatInputTextArea"]');
        if (chatInput) {
            chatInput.value = text;
            chatInput.dispatchEvent(new Event('input', { bubbles: true }));

            // ğŸ”¥ è‡ªå‹•ã§é€ä¿¡ï¼ˆEnterã‚­ãƒ¼ã‚’æŠ¼ã™ï¼‰
            const enterEvent = new KeyboardEvent('keydown', { key: 'Enter', bubbles: true });
            chatInput.dispatchEvent(enterEvent);
        }
    };

    recognition.onerror = (e) => {
        document.getElementById("mic-status").innerText = "âš ï¸ " + e.error;
    };
} else {
    document.write("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚");
}
</script>
<button onclick="startRec()">ğŸ™ è©±ã™</button>
<p id="mic-status">ãƒã‚¤ã‚¯åœæ­¢ä¸­</p>
""", height=130)

# ===============================
# ãƒãƒ£ãƒƒãƒˆç”»é¢
# ===============================
for msg in st.session_state.messages:
    avatar = "ğŸ§‘" if msg["role"] == "user" else "yukki-icon.jpg"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])

if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ§‘"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="yukki-icon.jpg"):
        with st.spinner("è€ƒãˆä¸­..."):
            response = st.session_state.chat.send_message(prompt)
            text = response.text
            st.markdown(text)
            st.info("ğŸ”Š éŸ³å£°å‡ºåŠ›ä¸­...")
            generate_and_play_tts(text)
            st.session_state.messages.append({"role": "assistant", "content": text})
