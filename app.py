import streamlit as st
from google import genai
import os
import base64
import json
import time
import requests
import streamlit.components.v1 as components

# -----------------------------------------------------
# ã€ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã€‘
# -----------------------------------------------------
SYSTEM_PROMPT = """
ã‚ãªãŸã¯æ•™è‚²çš„ãªç›®çš„ã‚’æŒã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ«ãƒ¼ãƒ«1ã€‘ äº‹å®Ÿãƒ»å®šç¾©ãªã©ã®è³ªå• â†’ ç›´æ¥ç°¡æ½”ã«ç­”ãˆã‚‹ã€‚
ã€ãƒ«ãƒ¼ãƒ«2ã€‘ æ€è€ƒãƒ»è¨ˆç®—ãƒ»è«–ç†ã®è³ªå• â†’ è§£æ³•ã®ãƒ’ãƒ³ãƒˆã®ã¿ã€‚
ã€ãƒ«ãƒ¼ãƒ«3ã€‘ é€”ä¸­å¼ã®ç¢ºèª â†’ æ­£ã—ã„ã‹ã©ã†ã‹ã ã‘è¿”ç­”ã€‚
"""

# -----------------------------------------------------
# --- å…±é€šè¨­å®š ---
# -----------------------------------------------------
TTS_API_URL = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent"
TTS_MODEL = "gemini-2.5-flash-preview-tts"
TTS_VOICE = "Kore"
MAX_RETRIES = 5

# --- APIã‚­ãƒ¼ ---
try:
    API_KEY = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloudã®ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()


# -----------------------------------------------------
# --- éŸ³å£°ã‚’è‡ªå‹•å†ç”Ÿã™ã‚‹é–¢æ•° ---
# -----------------------------------------------------
@st.cache_data
def base64_to_audio_url(base64_data, sample_rate):
    js_code = f"""
    <script>
        function base64ToArrayBuffer(base64) {{
            const binary_string = window.atob(base64);
            const len = binary_string.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {{
                bytes[i] = binary_string.charCodeAt(i);
            }}
            return bytes.buffer;
        }}

        function pcmToWav(pcmData, sampleRate) {{
            const numChannels = 1;
            const bitsPerSample = 16;
            const bytesPerSample = bitsPerSample / 8;
            const blockAlign = numChannels * bytesPerSample;
            const byteRate = sampleRate * blockAlign;
            const dataSize = pcmData.byteLength;
            const buffer = new ArrayBuffer(44 + dataSize);
            const view = new DataView(buffer);
            let offset = 0;

            function writeString(view, offset, string) {{
                for (let i = 0; i < string.length; i++) {{
                    view.setUint8(offset + i, string.charCodeAt(i));
                }}
            }}

            writeString(view, offset, 'RIFF'); offset += 4;
            view.setUint32(offset, 36 + dataSize, true); offset += 4;
            writeString(view, offset, 'WAVE'); offset += 4;
            writeString(view, offset, 'fmt '); offset += 4;
            view.setUint32(offset, 16, true); offset += 4;
            view.setUint16(offset, 1, true); offset += 2;
            view.setUint16(offset, numChannels, true); offset += 2;
            view.setUint32(offset, sampleRate, true); offset += 4;
            view.setUint32(offset, byteRate, true); offset += 4;
            view.setUint16(offset, blockAlign, true); offset += 2;
            view.setUint16(offset, bitsPerSample, true); offset += 2;
            writeString(view, offset, 'data'); offset += 4;
            view.setUint32(offset, dataSize, true); offset += 4;

            const pcm16 = new Int16Array(pcmData);
            for (let i = 0; i < pcm16.length; i++) {{
                view.setInt16(offset, pcm16[i], true);
                offset += 2;
            }}
            return new Blob([buffer], {{ type: 'audio/wav' }});
        }}

        const pcmData = base64ToArrayBuffer('{base64_data}');
        const wavBlob = pcmToWav(pcmData, {sample_rate});
        const audioUrl = URL.createObjectURL(wavBlob);
        const audio = new Audio(audioUrl);
        audio.play().catch(e => console.log("Audio autoplay failed:", e));
    </script>
    """
    components.html(js_code, height=0, width=0)


def generate_and_play_tts(text):
    payload = {
        "contents": [{"parts": [{"text": text}]}],
        "generationConfig": {
            "responseModalities": ["AUDIO"],
            "speechConfig": {"voiceConfig": {"prebuiltVoiceConfig": {"voiceName": TTS_VOICE}}},
        },
        "model": TTS_MODEL,
    }

    headers = {'Content-Type': 'application/json'}

    for attempt in range(MAX_RETRIES):
        try:
            response = requests.post(f"{TTS_API_URL}?key={API_KEY}", headers=headers, data=json.dumps(payload))
            response.raise_for_status()
            result = response.json()
            candidate = result.get('candidates', [{}])[0]
            part = candidate.get('content', {}).get('parts', [{}])[0]
            audio_data = part.get('inlineData', {})
            if audio_data and audio_data.get('data'):
                mime_type = audio_data.get('mimeType', 'audio/L16;rate=24000')
                sample_rate = int(mime_type.split('rate=')[1]) if 'rate=' in mime_type else 24000
                base64_to_audio_url(audio_data['data'], sample_rate)
                return True
            st.error("éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return False
        except Exception as e:
            st.error(f"TTSã‚¨ãƒ©ãƒ¼: {e}")
            return False
    return False


# -----------------------------------------------------
# --- éŸ³å£°å…¥åŠ›UI ---
# -----------------------------------------------------
def speech_to_text_ui():
    st.markdown("### ğŸ™ï¸ éŸ³å£°ã§è³ªå•ã™ã‚‹")
    html_code = """
    <script>
    let recognizing = false;
    let recognition;
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.lang = 'ja-JP';
        recognition.interimResults = false;
        recognition.continuous = false;

        function startRecognition() {
            if (!recognizing) {
                recognizing = true;
                recognition.start();
                document.getElementById('mic-status').innerText = 'ğŸ§ è´ãå–ã‚Šä¸­...';
            } else {
                recognizing = false;
                recognition.stop();
                document.getElementById('mic-status').innerText = 'ãƒã‚¤ã‚¯åœæ­¢ä¸­';
            }
        }

        recognition.onresult = function(event) {
            const transcript = event.results[0][0].transcript;
            const inputBox = window.parent.document.querySelector('textarea');
            if (inputBox) {
                inputBox.value = transcript;
                const enterEvent = new KeyboardEvent('keydown', {{ key: 'Enter', bubbles: true }});
                inputBox.dispatchEvent(enterEvent);
            }
            document.getElementById('mic-status').innerText = 'âœ… èªè­˜å®Œäº†: ' + transcript;
        };

        recognition.onerror = function(event) {
            document.getElementById('mic-status').innerText = 'âš ï¸ ã‚¨ãƒ©ãƒ¼: ' + event.error;
        };
    }
    </script>

    <button onclick="startRecognition()">ğŸ¤ è©±ã™ / åœæ­¢</button>
    <p id="mic-status">ãƒã‚¤ã‚¯åœæ­¢ä¸­</p>
    """
    components.html(html_code, height=120)


# -----------------------------------------------------
# --- Streamlitæœ¬ä½“ ---
# -----------------------------------------------------
st.set_page_config(page_title="ãƒ¦ãƒƒã‚­ãƒ¼", layout="wide")
st.title("ãƒ¦ãƒƒã‚­ãƒ¼")
st.caption("ç§ã¯å¯¾è©±å‹AIãƒ¦ãƒƒã‚­ãƒ¼ã ã‚ˆã€‚æ•°å­¦ã®å•é¡Œãªã©æ€è€ƒã™ã‚‹å•é¡Œã®ç­”ãˆã¯æ•™ãˆãªã„ã‹ã‚‰ã­ğŸ’•")

if "client" not in st.session_state:
    st.session_state.client = genai.Client(api_key=API_KEY)

if "chat" not in st.session_state:
    config = {"system_instruction": SYSTEM_PROMPT, "temperature": 0.2}
    st.session_state.chat = st.session_state.client.chats.create(model='gemini-2.5-flash', config=config)

USER_AVATAR = "ğŸ§‘"
AI_AVATAR = "yukki-icon.jpg"

if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´
for message in st.session_state.messages:
    avatar = USER_AVATAR if message["role"] == "user" else AI_AVATAR
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# ğŸ¤ éŸ³å£°å…¥åŠ›UI
speech_to_text_ui()

# --- å…¥åŠ›å‡¦ç† ---
if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar=USER_AVATAR):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar=AI_AVATAR):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                response = st.session_state.chat.send_message(prompt)
                response_text = response.text.strip()
                st.markdown(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
                st.info("ğŸ”Š éŸ³å£°å¿œç­”ã‚’æº–å‚™ä¸­...")
                generate_and_play_tts(response_text)
            except Exception as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {e}")
